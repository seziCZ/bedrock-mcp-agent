from typing import List, Optional, Dict
from urllib.parse import urlparse

from langchain.agents import create_agent
from langchain_aws import ChatBedrockConverse
from langchain_core.messages import HumanMessage
from langchain_mcp_adapters.client import MultiServerMCPClient


class Agent:
    """
    Lightweight wrapper around a LangChain agent that integrates AWS Bedrock as the LLM
    and discovers tools from one or more MCP servers. It provides a simple `invoke`
    method for executing natural-language prompts using the configured model and tools.
    """

    def __init__(
            self,
            mcp_endpoints: List[str],
            mcp_headers: Optional[Dict[str, str]] = None,
            llm_model: str = "global.amazon.nova-2-lite-v1:0",
            llm_model_temperature: float = 0.2,
            llm_model_max_tokens: int = 512,
    ) -> None:
        """
        Constructor.
        :param mcp_endpoints: List of MCP endpoint URLs to connect to.
        :param mcp_headers: Optional HTTP headers applied to all MCP requests (e.g. auth).
        :param llm_model: AWS Bedrock model identifier.
        :param llm_model_temperature: Sampling temperature for the LLM.
        :param llm_model_max_tokens: Maximum number of tokens to generate in a response.
        """

        # initialize Bedrock client
        self.llm = ChatBedrockConverse(
            model=llm_model,
            temperature=llm_model_temperature,
            max_tokens=llm_model_max_tokens
        )

        # initialize MCP client
        # see https://pypi.org/project/langchain-mcp-adapters/
        self.mcp = MultiServerMCPClient({
            urlparse(mcp_endpoint).hostname: {
                "transport": "streamable_http",
                "headers": mcp_headers or {},
                "url": mcp_endpoint,
            }
            for mcp_endpoint
            in mcp_endpoints
        })


    async def invoke(self, prompt: str) -> str:
        """
        Executes the provided prompt against a LangChain agent backed by AWS Bedrock and MCP tools.
        # TODO: Persist conversation history using a ledger or vector store
        :param prompt: User-defined natural language prompt to be processed by the agent.
        :return: Textual response generated by the agent.
        """

        # initialize agent, through LangChain
        tools = await self.mcp.get_tools()
        agent = create_agent(
            model=self.llm,
            tools=tools,
            debug=True
        )

        # invoke the agent with the user provided input
        responses = await agent.ainvoke({ # type: ignore[arg-type]
            "messages": [
                HumanMessage(
                    content=prompt
                )
            ]
        })

        # return the most recent message content
        last_message = responses["messages"][-1]
        return getattr(last_message, "content", "")
